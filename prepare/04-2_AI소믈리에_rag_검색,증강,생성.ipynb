{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ca1ca4-237c-4279-96ec-558ffc23e28b",
   "metadata": {},
   "source": [
    "## RAG로 AI소믈리에 wine pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf3c202c-2a10-4529-86af-06514492450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "PINECONE_NAMESPACE = os.getenv(\"PINECONE_NAMESPACE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "411f1f3b-8c8f-42e8-b126-d687c4125f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ae29f-600a-4620-a4f0-b4d188ff66b8",
   "metadata": {},
   "source": [
    "### LLM을 통한 요리 정보 해석\n",
    "- 이미지 (image to text)\n",
    "- 입력 : url\n",
    "- 출력 : 요리명, 설명\n",
    "- 함수로 정의, runnableLambda 객체 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d1cff3a-ef9c-470b-bc77-9f8803a24cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_dish_flavor(input_data):\n",
    "\n",
    "    prompt = ChatPromptTemplate([\n",
    "        (\"system\" , \"\"\"\n",
    "        # Role\n",
    "    당신은 세계적인 수준의 '디지털 소믈리에'입니다. 사용자가 제공한 음식 사진을 시각적으로 분석하여, 해당 요리의 특징을 도출하고 최적의 와인 페어링을 제안하는 것이 당신의 임무입니다.\n",
    "    \n",
    "    # Analysis Process (사고 단계)\n",
    "    1. **시각적 요소 식별**: 주재료(단백질 종류), 조리 방식(굽기, 찌기, 튀기기), 소스의 색상 및 질감, 곁들임 채소(가니쉬)를 파악합니다.\n",
    "    2. **맛 프로파일 유추**: 시각 정보를 바탕으로 예상되는 [염도, 산도, 당도, 지방함량, 질감, 향신료 강도]를 수치화하거나 상세히 묘사합니다.\n",
    "    3. **페어링 전략 설정**: '보완(Congruent)' 또는 '대비(Contrast)' 전략 중 무엇을 사용할지 결정합니다.\n",
    "    4. **와인 그룹 선정**: 구체적인 품종이나 지역을 포함한 최적의 와인 그룹을 추천합니다.\n",
    "    \n",
    "    # Output Format (응답 형식)\n",
    "    1. **요리 분석**: (예: 노릇하게 구워진 마이야르 반응이 특징인 소고기 스테이크, 진한 레드와인 소스)\n",
    "    2. **예상되는 맛**: (예: 높은 단백질 농도, 버터의 풍미, 후추의 매콤함)\n",
    "    3. **추천 와인 카테고리**: (예: 풀 바디 레드 와인)\n",
    "    4. **추천 품종 및 지역**: (예: 프랑스 보르도의 카베르네 소비뇽 또는 호주의 쉬라즈)\n",
    "    5. **페어링 이유**: (전문 소믈리에의 언어로 설명)\n",
    "    6. **서빙 팁**: (적정 온도 및 디캔팅 여부)\n",
    "    \"\"\"),\n",
    "        HumanMessagePromptTemplate.from_template([\n",
    "            {\"text\": \"이 음식에 맞는 와인을 찾을 수 있도록 이 음식을 분석해 주세요\"},\n",
    "            {\"image_url\": \"{image_url}\"} # image_url는 정해줘 있음.\n",
    "        ])\n",
    "    ])\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "                        temperature=0.2,\n",
    "                        google_api_key = GEMINI_API_KEY)\n",
    "\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    return chain    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d916997-df69-4794-836b-12b062a60cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "r1 = RunnableLambda(desc_dish_flavor)\n",
    "input_data = { \"image_url\" :\n",
    "    \"https://upload3.inven.co.kr/upload/2025/12/31/bbs/i1701205734.jpg\"}\n",
    "\n",
    "response = r1.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e79c831c-bac7-4396-9348-2f650affcc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'훌륭한 돼지고기 구이 사진입니다! 제가 분석하여 최적의 와인 페어링을 제안해 드리겠습니다.\\n\\n---\\n\\n1.  **요리 분석**:\\n    뼈가 붙은 삼겹살을 노릇하게 구워낸 요리로, 겉은 바삭한 마이야르 반응이 선명하고 속은 촉촉해 보이는 돼지고기 구이입니다. 고명으로 깨와 다진 파(추정)가 올라가 있어, 과도하게 달거나 짜지 않은 담백한 양념의 구이임을 시사합니다.\\n\\n2.  **예상되는 맛**:\\n    삼겹살 특유의 높은 지방 함량에서 오는 풍부한 고소함과 감칠맛이 도드라질 것입니다. 구운 돼지고기의 육향과 은은한 캐러멜화된 풍미가 특징이며, 겉은 바삭하고 속은 부드러운 질감이 예상됩니다. 강한 향신료보다는 고기 본연의 맛을 살린 간이 주를 이룰 것으로 보입니다.\\n\\n3.  **추천 와인 카테고리**:\\n    미디엄-풀 바디 레드 와인 (Medium-Full Bodied Red Wine)\\n\\n4.  **추천 품종 및 지역**:\\n    *   **쉬라즈(Syrah/Shiraz)**: 호주(Shiraz) 또는 프랑스 론 밸리(Syrah)\\n    *   **템프라니요(Tempranillo)**: 스페인 리오하(Rioja)\\n\\n5.  **페어링 이유**:\\n    이 삼겹살 구이는 풍부한 지방과 고소한 풍미가 핵심이므로, 와인의 **적절한 산도와 탄닌이 지방의 느끼함을 깔끔하게 씻어주고, 와인의 농축된 과일향과 스파이시함이 구이의 고소하고 진한 맛을 보완하고 증폭시키는 보완(Congruent) 및 대비(Contrast) 전략**을 활용합니다.\\n\\n    *   **쉬라즈/시라**: 짙은 과일향(블랙베리, 플럼)과 후추 같은 스파이시함, 그리고 묵직한 바디감을 가진 쉬라즈/시라는 삼겹살의 기름진 맛을 견고한 구조감으로 잡아줍니다. 구운 고기의 스모키한 뉘앙스와 와인의 오크 숙성에서 오는 복합적인 향이 훌륭하게 어우러져, 서로의 풍미를 더욱 깊게 만들어줍니다.\\n    *   **템프라니요**: 특히 오크 숙성된 스페인 리오하의 템프라니요는 체리, 자두 등의 붉은 과일향과 함께 가죽, 담배, 바닐라 같은 복합적인 아로마를 발산합니다. 부드러우면서도 존재감 있는 탄닌과 적절한 산미는 삼겹살의 지방을 효과적으로 중화시키고, 숙성 와인의 깊이감이 구운 고기의 감칠맛을 한층 더 끌어올려줍니다.\\n\\n6.  **서빙 팁**:\\n    *   **적정 온도**: 두 품종 모두 16~18°C 정도로 서빙하는 것이 좋습니다. 너무 차가우면 와인의 풍미가 닫히고, 너무 따뜻하면 알코올이 도드라질 수 있습니다.\\n    *   **디캔팅 여부**: 영한 쉬라즈나 리오하 크리안자(Crianza)급 템프라니요라면 30분 정도의 디캔팅으로 와인의 향을 열어주는 것이 좋습니다. 리오하 레제르바(Reserva) 또는 그란 레제르바(Gran Reserva)와 같은 장기 숙성 템프라니요는 1시간 이상 디캔팅하여 복합적인 아로마와 침전물을 걸러내면 더욱 깊은 맛을 즐길 수 있습니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81a13764-ff3e-42a1-b0be-45886c83e4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "훌륭한 돼지고기 구이 사진입니다! 제가 분석하여 최적의 와인 페어링을 제안해 드리겠습니다.\n",
       "\n",
       "---\n",
       "\n",
       "1.  **요리 분석**:\n",
       "    뼈가 붙은 삼겹살을 노릇하게 구워낸 요리로, 겉은 바삭한 마이야르 반응이 선명하고 속은 촉촉해 보이는 돼지고기 구이입니다. 고명으로 깨와 다진 파(추정)가 올라가 있어, 과도하게 달거나 짜지 않은 담백한 양념의 구이임을 시사합니다.\n",
       "\n",
       "2.  **예상되는 맛**:\n",
       "    삼겹살 특유의 높은 지방 함량에서 오는 풍부한 고소함과 감칠맛이 도드라질 것입니다. 구운 돼지고기의 육향과 은은한 캐러멜화된 풍미가 특징이며, 겉은 바삭하고 속은 부드러운 질감이 예상됩니다. 강한 향신료보다는 고기 본연의 맛을 살린 간이 주를 이룰 것으로 보입니다.\n",
       "\n",
       "3.  **추천 와인 카테고리**:\n",
       "    미디엄-풀 바디 레드 와인 (Medium-Full Bodied Red Wine)\n",
       "\n",
       "4.  **추천 품종 및 지역**:\n",
       "    *   **쉬라즈(Syrah/Shiraz)**: 호주(Shiraz) 또는 프랑스 론 밸리(Syrah)\n",
       "    *   **템프라니요(Tempranillo)**: 스페인 리오하(Rioja)\n",
       "\n",
       "5.  **페어링 이유**:\n",
       "    이 삼겹살 구이는 풍부한 지방과 고소한 풍미가 핵심이므로, 와인의 **적절한 산도와 탄닌이 지방의 느끼함을 깔끔하게 씻어주고, 와인의 농축된 과일향과 스파이시함이 구이의 고소하고 진한 맛을 보완하고 증폭시키는 보완(Congruent) 및 대비(Contrast) 전략**을 활용합니다.\n",
       "\n",
       "    *   **쉬라즈/시라**: 짙은 과일향(블랙베리, 플럼)과 후추 같은 스파이시함, 그리고 묵직한 바디감을 가진 쉬라즈/시라는 삼겹살의 기름진 맛을 견고한 구조감으로 잡아줍니다. 구운 고기의 스모키한 뉘앙스와 와인의 오크 숙성에서 오는 복합적인 향이 훌륭하게 어우러져, 서로의 풍미를 더욱 깊게 만들어줍니다.\n",
       "    *   **템프라니요**: 특히 오크 숙성된 스페인 리오하의 템프라니요는 체리, 자두 등의 붉은 과일향과 함께 가죽, 담배, 바닐라 같은 복합적인 아로마를 발산합니다. 부드러우면서도 존재감 있는 탄닌과 적절한 산미는 삼겹살의 지방을 효과적으로 중화시키고, 숙성 와인의 깊이감이 구운 고기의 감칠맛을 한층 더 끌어올려줍니다.\n",
       "\n",
       "6.  **서빙 팁**:\n",
       "    *   **적정 온도**: 두 품종 모두 16~18°C 정도로 서빙하는 것이 좋습니다. 너무 차가우면 와인의 풍미가 닫히고, 너무 따뜻하면 알코올이 도드라질 수 있습니다.\n",
       "    *   **디캔팅 여부**: 영한 쉬라즈나 리오하 크리안자(Crianza)급 템프라니요라면 30분 정도의 디캔팅으로 와인의 향을 열어주는 것이 좋습니다. 리오하 레제르바(Reserva) 또는 그란 레제르바(Gran Reserva)와 같은 장기 숙성 템프라니요는 1시간 이상 디캔팅하여 복합적인 아로마와 침전물을 걸러내면 더욱 깊은 맛을 즐길 수 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068ee6c-5489-4f12-b5ab-9fa7b7da5b17",
   "metadata": {},
   "source": [
    "## 요리에 가장 잘 어울리는 wine top 5 검색\n",
    "- pinecone : vector db 저장 되어있음\n",
    "- index : winereviews, namespace : wine_reviews-ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deafc97c-03fe-46e6-9284-c17225ffa4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"winereviews\"\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c3871e4-2c97-4f5d-868d-3fd5be686cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model = OPENAI_EMBEDDING_MODEL,\n",
    "    api_key = OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25c66453-d873-4e1d-9648-f4d80058a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# pineconevectorstore 객체 생성\n",
    "vector_store = PineconeVectorStore(index=index,        \n",
    "                                   embedding=embedding, \n",
    "                                   text_key=\"text\"   # text키 =db 저장text\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab5617f8-7b26-45f5-ac78-e25a4719cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bc31874-fd6c-474c-afb0-95c55ac08b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m question = response\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# query는 텍스트 형식으로 입력\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPINECONE_NAMESPACE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:676\u001b[39m, in \u001b[36mPineconeVectorStore.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, namespace, **kwargs)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    658\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    659\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    663\u001b[39m     **kwargs: Any,\n\u001b[32m    664\u001b[39m ) -> List[Document]:\n\u001b[32m    665\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return pinecone documents most similar to query.\u001b[39;00m\n\u001b[32m    666\u001b[39m \n\u001b[32m    667\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    674\u001b[39m \u001b[33;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    679\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:543\u001b[39m, in \u001b[36mPineconeVectorStore.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, namespace, **kwargs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    524\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    525\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    529\u001b[39m     **kwargs: Any,\n\u001b[32m    530\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    531\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return pinecone documents most similar to query, along with scores.\u001b[39;00m\n\u001b[32m    532\u001b[39m \n\u001b[32m    533\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    540\u001b[39m \u001b[33;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.similarity_search_by_vector_with_score(\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    544\u001b[39m         k=k,\n\u001b[32m    545\u001b[39m         \u001b[38;5;28mfilter\u001b[39m=\u001b[38;5;28mfilter\u001b[39m,\n\u001b[32m    546\u001b[39m         namespace=namespace,\n\u001b[32m    547\u001b[39m         **kwargs,\n\u001b[32m    548\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:759\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m    750\u001b[39m \n\u001b[32m    751\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    756\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    757\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;28mself\u001b[39m._ensure_sync_client_available()\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:709\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    708\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:576\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    575\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    578\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "question = response\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=question,    # query는 텍스트 형식으로 입력\n",
    "    k=5, \n",
    "    namespace=PINECONE_NAMESPACE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "lc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
