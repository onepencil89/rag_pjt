{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d684e7",
   "metadata": {},
   "source": [
    "## runnableLambda 객체 사용방법 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30b9cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "# GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ebe0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc36dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의\n",
    "def func(x):\n",
    "    return  x*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b130bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 전달인자로 func 넣기\n",
    "\n",
    "r1 = RunnableLambda(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00468867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = 5\n",
    "r1.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd088fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = RunnableLambda(lambda x: x*2)\n",
    "r1.invoke(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2834b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 40]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# runnable batch\n",
    "\n",
    "r3 = RunnableLambda(func)\n",
    "\n",
    "input_data = [5, 10, 20]\n",
    "\n",
    "r3.batch(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226b12c",
   "metadata": {},
   "source": [
    "### 순차적으로 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c515cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "r1 = RunnableLambda(lambda x: 3 * x)\n",
    "r2 = RunnableLambda(lambda x: x + 5)\n",
    "\n",
    "chain = r1 | r2\n",
    "\n",
    "chain.invoke(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11fdf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "r1 = RunnableLambda(lambda x: 3 * x)\n",
    "r2 = RunnableLambda(lambda x: x + 5)\n",
    "\n",
    "# chain = r1 | r2  # 아래와 같은 표현\n",
    "chain = RunnableSequence(r1, r2)\n",
    "\n",
    "chain.invoke(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d01bc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 30, 'second': 15}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "r1 = RunnableLambda(lambda x: 3 * x)\n",
    "r2 = RunnableLambda(lambda x: x + 5)\n",
    "\n",
    "# 병렬처리 , r1, r2 키는 임으로 설정해 주면 됨.\n",
    "# chain = RunnableParallel(r1=r1, r2=r2)\n",
    "chain = RunnableParallel(first=r1, second=r2)\n",
    "\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "619d74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c6a769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'foo': 2}, {'foo': 2}, {'foo': 2}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\"foo\":2}]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba10a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r2': [{'foo': 2}, {'foo': 2}, {'foo': 2}], 'r3': \"{'foo': 2}\"}\n",
      "        +-------------+        \n",
      "        | LambdaInput |        \n",
      "        +-------------+        \n",
      "               *               \n",
      "               *               \n",
      "               *               \n",
      "          +--------+           \n",
      "          | Lambda |           \n",
      "          +--------+           \n",
      "               *               \n",
      "               *               \n",
      "               *               \n",
      "   +----------------------+    \n",
      "   | Parallel<r2,r3>Input |    \n",
      "   +----------------------+    \n",
      "          *         *          \n",
      "        **           **        \n",
      "       *               *       \n",
      "+--------+          +--------+ \n",
      "| Lambda |          | Lambda | \n",
      "+--------+          +--------+ \n",
      "          *         *          \n",
      "           **     **           \n",
      "             *   *             \n",
      "  +-----------------------+    \n",
      "  | Parallel<r2,r3>Output |    \n",
      "  +-----------------------+    \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "runnable_1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable_2 = RunnableLambda(lambda x: [x] * 3)\n",
    "runnable_3 = RunnableLambda(lambda x: str(x))\n",
    "\n",
    "chain = runnable_1 | RunnableParallel(r2=runnable_2, r3=runnable_3)\n",
    "\n",
    "print(chain.invoke(2))\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb3861",
   "metadata": {},
   "source": [
    "## LCEL 문법 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e20786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b86faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "                        temperature=0.2,\n",
    "                        google_api_key = GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6161e3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='주어진 컨텍스트(\"Vector Search에 의한 컨텐스트 내용\")는 질문 \"안녕\"에 대한 답변을 포함하고 있지 않습니다.\\n\\n따라서 해당 컨텍스트 내에서는 \"안녕\"이라는 질문에 답할 수 없습니다.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b8d10-ce2f-73a0-8b56-781488709e64-0' usage_metadata={'input_tokens': 39, 'output_tokens': 556, 'total_tokens': 595, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 505}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    다음 컨텍스트에 대해서만 답하세요.\n",
    "    컨텍스트 :\n",
    "    {context}\n",
    "    질문:\n",
    "    {query}\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "input = {\n",
    "            \"context\": \"Vector Search에 의한 컨텐스트 내용\", \n",
    "            \"query\": \"안녕\"\n",
    "        }\n",
    "print(chain.invoke(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9214c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    다음 컨텍스트에 대해서만 답하세요.\n",
    "    컨텍스트 :\n",
    "    {context}\n",
    "    질문:\n",
    "    {query}\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "input = {\n",
    "            \"context\": \"Vector Search에 의한 컨텐스트 내용\", \n",
    "            \"query\": \"안녕\"\n",
    "        }\n",
    "print(chain.invoke(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92073afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    다음 컨텍스트에 대해서만 답하세요.\n",
    "    컨텍스트 :\n",
    "    {context}\n",
    "    질문:\n",
    "    {query}\n",
    "\"\"\")\n",
    "\n",
    "# 기본값을 제공하는 함수\n",
    "def add_default_values(input_dict):\n",
    "    return {\n",
    "        \"context\": input_dict.get(\"context\", \"너는 Langchain에 대해 설명해주는 AI전문 강사야\"),\n",
    "        \"query\": input_dict.get(\"query\", \"나는 왜 langchain이 개발되었는지 궁금해\")\n",
    "    }\n",
    "\n",
    "\n",
    "# 체인 구성\n",
    "chain = RunnableLambda(add_default_values) | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9660944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='아주 좋은 질문입니다! 왜 LangChain이 개발되었는지 이해하는 것이 이 프레임워크의 본질을 파악하는 데 가장 중요합니다.\\n\\n자, 제가 AI 전문 강사의 입장에서 명확하게 설명해 드릴게요.\\n\\nLLM(대규모 언어 모델)은 그 자체로 엄청나게 강력한 \\'두뇌\\'입니다. 자연어를 이해하고, 생성하고, 다양한 지식과 추론 능력을 가지고 있죠. 하지만 이 두뇌를 현실 세계의 다양한 정보와 연결하고, 특정 작업을 수행하도록 지시하며, 여러 단계를 거쳐 복잡한 문제를 해결하게 만드는 것은 별개의 이야기였습니다.\\n\\nLangChain이 개발된 주요 이유는 다음과 같은 \"갭(Gap)\"을 채우기 위함입니다:\\n\\n1.  **LLM의 한계:**\\n    *   **외부 정보의 부재:** LLM은 훈련받은 데이터 내의 지식만 가지고 있습니다. 실시간 웹 정보, 기업 내부 데이터베이스, 사용자의 개인 파일 등 외부의 최신 정보를 직접 조회하거나 활용할 수 없습니다.\\n    *   **도구 사용의 어려움:** LLM은 계산기, 검색 엔진, API 호출 등 특정 기능을 수행하는 외부 \\'도구\\'를 직접 사용할 수 없습니다. 개발자가 수동으로 LLM의 응답을 파싱하고, 도구를 호출하고, 그 결과를 다시 LLM에게 넘겨주는 복잡한 과정을 거쳐야 했습니다.\\n    *   **다단계 작업의 복잡성:** LLM은 한 번의 질문에 답하는 데는 능숙하지만, 여러 단계를 거쳐야 하는 복잡한 작업 (예: \"이메일 초안 작성 후 관련 정보 검색해서 첨부해줘\")을 수행하게 하는 것은 어려웠습니다. 각 단계를 수동으로 연결해야 했죠.\\n    *   **기억 (Memory)의 부재:** LLM은 기본적으로 \\'상태 비저장(stateless)\\'입니다. 이전 대화의 맥락을 기억하지 못하므로, 장기적인 대화를 이어가기 위해서는 개발자가 직접 대화 이력을 관리하고 프롬프트에 주입해야 했습니다.\\n\\n2.  **개발 효율성의 문제:**\\n    *   위와 같은 문제를 해결하기 위해 개발자들은 매번 복잡한 파이프라인을 직접 코딩해야 했습니다. 이는 시간 소모적이고, 오류 발생 가능성이 높으며, 재사용하기 어려운 코드를 양산했습니다.\\n\\n**결론적으로, LangChain은 LLM을 활용한 애플리케이션 개발을 위한 일종의 \\'운영 체제\\' 또는 \\'프레임워크\\' 역할을 합니다.**\\n\\nLLM을 단순히 텍스트를 생성하는 도구를 넘어, 외부 환경과 상호작용하고, 기억하며, 능동적으로 문제를 해결하는 지능형 시스템으로 변모시키는 다리 역할을 하는 것이죠. LangChain은 이 과정을 쉽게 만들어주는 모듈화된 구성 요소와 표준화된 인터페이스를 제공하여, 개발자들이 LLM의 잠재력을 최대한 발휘하는 강력하고 복잡한 애플리케이션을 더 쉽고 빠르게 구축할 수 있도록 돕기 위해 탄생했습니다.\\n\\n이해되셨나요? LangChain 덕분에 우리는 LLM으로 훨씬 더 역동적이고 유용한 것들을 만들 수 있게 된 겁니다.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b8d17-2c78-77a2-a16c-4b73d8cb9f39-0' usage_metadata={'input_tokens': 52, 'output_tokens': 2341, 'total_tokens': 2393, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1644}}\n"
     ]
    }
   ],
   "source": [
    "# 빈 딕셔너리로도 실행 가능\n",
    "result = chain.invoke({})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f7d2aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "아주 좋은 질문입니다! 왜 LangChain이 개발되었는지 이해하는 것이 이 프레임워크의 본질을 파악하는 데 가장 중요합니다.\n",
       "\n",
       "자, 제가 AI 전문 강사의 입장에서 명확하게 설명해 드릴게요.\n",
       "\n",
       "LLM(대규모 언어 모델)은 그 자체로 엄청나게 강력한 '두뇌'입니다. 자연어를 이해하고, 생성하고, 다양한 지식과 추론 능력을 가지고 있죠. 하지만 이 두뇌를 현실 세계의 다양한 정보와 연결하고, 특정 작업을 수행하도록 지시하며, 여러 단계를 거쳐 복잡한 문제를 해결하게 만드는 것은 별개의 이야기였습니다.\n",
       "\n",
       "LangChain이 개발된 주요 이유는 다음과 같은 \"갭(Gap)\"을 채우기 위함입니다:\n",
       "\n",
       "1.  **LLM의 한계:**\n",
       "    *   **외부 정보의 부재:** LLM은 훈련받은 데이터 내의 지식만 가지고 있습니다. 실시간 웹 정보, 기업 내부 데이터베이스, 사용자의 개인 파일 등 외부의 최신 정보를 직접 조회하거나 활용할 수 없습니다.\n",
       "    *   **도구 사용의 어려움:** LLM은 계산기, 검색 엔진, API 호출 등 특정 기능을 수행하는 외부 '도구'를 직접 사용할 수 없습니다. 개발자가 수동으로 LLM의 응답을 파싱하고, 도구를 호출하고, 그 결과를 다시 LLM에게 넘겨주는 복잡한 과정을 거쳐야 했습니다.\n",
       "    *   **다단계 작업의 복잡성:** LLM은 한 번의 질문에 답하는 데는 능숙하지만, 여러 단계를 거쳐야 하는 복잡한 작업 (예: \"이메일 초안 작성 후 관련 정보 검색해서 첨부해줘\")을 수행하게 하는 것은 어려웠습니다. 각 단계를 수동으로 연결해야 했죠.\n",
       "    *   **기억 (Memory)의 부재:** LLM은 기본적으로 '상태 비저장(stateless)'입니다. 이전 대화의 맥락을 기억하지 못하므로, 장기적인 대화를 이어가기 위해서는 개발자가 직접 대화 이력을 관리하고 프롬프트에 주입해야 했습니다.\n",
       "\n",
       "2.  **개발 효율성의 문제:**\n",
       "    *   위와 같은 문제를 해결하기 위해 개발자들은 매번 복잡한 파이프라인을 직접 코딩해야 했습니다. 이는 시간 소모적이고, 오류 발생 가능성이 높으며, 재사용하기 어려운 코드를 양산했습니다.\n",
       "\n",
       "**결론적으로, LangChain은 LLM을 활용한 애플리케이션 개발을 위한 일종의 '운영 체제' 또는 '프레임워크' 역할을 합니다.**\n",
       "\n",
       "LLM을 단순히 텍스트를 생성하는 도구를 넘어, 외부 환경과 상호작용하고, 기억하며, 능동적으로 문제를 해결하는 지능형 시스템으로 변모시키는 다리 역할을 하는 것이죠. LangChain은 이 과정을 쉽게 만들어주는 모듈화된 구성 요소와 표준화된 인터페이스를 제공하여, 개발자들이 LLM의 잠재력을 최대한 발휘하는 강력하고 복잡한 애플리케이션을 더 쉽고 빠르게 구축할 수 있도록 돕기 위해 탄생했습니다.\n",
       "\n",
       "이해되셨나요? LangChain 덕분에 우리는 LLM으로 훨씬 더 역동적이고 유용한 것들을 만들 수 있게 된 겁니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35e58854",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = chain.invoke({\"query\": \"RunnableLambda에 대해 1문장으로 설명해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89e79dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제공된 컨텍스트에는 RunnableLambda에 대한 정보가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "print(result2.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
